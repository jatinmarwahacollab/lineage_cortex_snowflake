{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "bf169a0e-eef4-4b72-a48a-530fce9cc831",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark as sp\nfrom sqlglot import parse_one, exp\nimport re\n\n\n# Use the provided session instead of creating a new one\ndef get_snowflake_session():\n    # Return the active session (provided by Snowflake environment)\n    return sp.Session.builder.getOrCreate()\n\n\n\n# Create the results table to store the processed data\ndef create_results_table(session):\n    session.sql(\"\"\"\n        CREATE TABLE IF NOT EXISTS PROCESSED_SQL_DATA (\n            DATABASE_NAME VARCHAR,\n            SCHEMA_NAME VARCHAR,\n            TABLE_NAME VARCHAR,\n            QUERY_TEXT VARCHAR,\n            EXPANDED_SQL VARCHAR\n        )\n    \"\"\").collect()\n\n# Fetch SQL text from a Snowflake view\ndef fetch_query_text(session):\n    result = session.sql(\"\"\"\n        SELECT DISTINCT table_name, query_text FROM VIEW_SNOWFLAKE_SQL_DATA\n    \"\"\").collect()\n    return [{'table_name': row['TABLE_NAME'], 'query_text': row['QUERY_TEXT']} for row in result]\n\n# Extract SELECT part from DDL/DML\ndef extract_select_from_ddl(query_text):\n    pattern = re.compile(r\"\\bAS\\s*\\(\\s*\", re.IGNORECASE)\n    match = pattern.search(query_text)\n    if not match:\n        print(\"No 'AS (' pattern found in the query.\")\n        return None\n\n    start_pos = match.end()\n    end_pos = query_text.rfind(')')\n    if end_pos == -1:\n        print(\"No matching closing parenthesis found.\")\n        return None\n\n    extracted_sql = query_text[start_pos:end_pos].strip()\n    if not extracted_sql.lower().startswith((\"select\", \"with\")):\n        print(\"Extracted SQL does not start with 'SELECT' or 'WITH'.\")\n        return None\n\n    return extracted_sql\n\n# Fetch table columns using Snowpark DataFrame\ndef get_table_columns(session, database, schema, table):\n    query = f\"\"\"\n        SELECT COLUMN_NAME\n        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n        WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'\n        ORDER BY ORDINAL_POSITION\n    \"\"\"\n    result = session.sql(query).collect()\n    return [row['COLUMN_NAME'] for row in result]\n\n# Generate expanded SQL\ndef generate_expanded_sql(original_sql, session):\n    parsed = parse_one(original_sql)\n    cte_columns = {}\n\n    def get_fully_qualified_name(table_expression):\n        parts = []\n        if table_expression.args.get(\"catalog\"):\n            parts.append(table_expression.args[\"catalog\"].name)\n        if table_expression.args.get(\"db\"):\n            parts.append(table_expression.args[\"db\"].name)\n        if table_expression.args.get(\"this\"):\n            parts.append(table_expression.args[\"this\"].name)\n        return \".\".join(parts)\n\n    def replace_star_in_select(select_expr, current_columns, all_columns):\n        new_expressions = []\n        for projection in select_expr.expressions:\n            if isinstance(projection, exp.Star):\n                for col in current_columns:\n                    new_expressions.append(exp.to_identifier(col))\n            elif isinstance(projection, exp.Column) and projection.args.get('table'):\n                table_name = projection.args['table'].name\n                if projection.name == \"*\":\n                    if table_name in all_columns:\n                        for col in all_columns[table_name]:\n                            new_expressions.append(exp.column(col, table=table_name))\n                    else:\n                        database, schema, table = split_table_name(table_name)\n                        columns = get_table_columns(session, database, schema, table)\n                        for col in columns:\n                            new_expressions.append(exp.column(col, table=table_name))\n                else:\n                    new_expressions.append(projection)\n            else:\n                new_expressions.append(projection)\n        select_expr.set(\"expressions\", new_expressions)\n\n    with_expression = parsed.args.get(\"with\")\n    if with_expression:\n        for cte in with_expression.expressions:\n            cte_name = cte.alias\n            select_expr = cte.this\n            from_expr = select_expr.args.get(\"from\")\n            from_table = from_expr.find(exp.Table)\n            source_full = get_fully_qualified_name(from_table)\n\n            if source_full in cte_columns:\n                source_columns = cte_columns[source_full]\n            else:\n                database, schema, table = split_table_name(source_full)\n                source_columns = get_table_columns(session, database, schema, table)\n\n            replace_star_in_select(select_expr, source_columns, cte_columns)\n            new_columns = [proj.sql() for proj in select_expr.expressions]\n            cte_columns[cte_name] = new_columns\n\n    final_select = parsed.find(exp.Select)\n    from_expr = final_select.args.get(\"from\")\n    from_table = from_expr.find(exp.Table)\n    source_full = get_fully_qualified_name(from_table)\n\n    if source_full in cte_columns:\n        final_columns = cte_columns[source_full]\n    else:\n        database, schema, table = split_table_name(source_full)\n        final_columns = get_table_columns(session, database, schema, table)\n\n    replace_star_in_select(final_select, final_columns, cte_columns)\n    return parsed.sql(pretty=True).upper()\n\ndef split_table_name(full_name):\n    parts = full_name.split('.')\n    if len(parts) == 3:\n        return parts\n    elif len(parts) == 2:\n        return os.getenv('database'), parts[0], parts[1]\n    elif len(parts) == 1:\n        return os.getenv('database'), os.getenv('schema'), parts[0]\n    raise ValueError(f\"Invalid table name: {full_name}\")\n\ndef insert_processed_data(session, database, schema, table, query, expanded_sql):\n    \"\"\"\n    Insert processed data into the PROCESSED_SQL_DATA table safely by treating \n    expanded SQL as a string using parameter binding.\n    \"\"\"\n    # Prepare the insertion query with placeholders\n    insert_query = \"\"\"\n        INSERT INTO PROCESSED_SQL_DATA \n        (DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, QUERY_TEXT, EXPANDED_SQL)\n        VALUES (?, ?, ?, ?, ?)\n    \"\"\"\n    \n    # Execute the query with parameterized binding\n    session.sql(\n        insert_query, \n        (database, schema, table, query, expanded_sql)\n    ).collect()\n\n\ndef main():\n\n    \n    session = get_snowflake_session()\n    result = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_ROLE()\").collect()\n    print(result)\n    create_results_table(session)\n    \n\n    sql_data = fetch_query_text(session)\n    session.sql(\"TRUNCATE TABLE PROCESSED_SQL_DATA\").collect()\n\n    for record in sql_data:\n        database, schema, table = split_table_name(record['table_name'])\n        query_text = record['query_text']\n        extracted_sql = extract_select_from_ddl(query_text)\n\n        if extracted_sql:\n            expanded_sql = generate_expanded_sql(extracted_sql, session)\n            insert_processed_data(session, database, schema, table, query_text, expanded_sql)\n\nif __name__ == \"__main__\":\n    main()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "source": "from snowflake.snowpark.session import Session\nfrom snowflake.snowpark.functions import call_builtin\nimport json\nimport re\nimport logging\n\n# Configure logging for better debugging and visibility\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n\n# Get the active Snowpark session (in a Snowflake notebook, the session is usually already available)\nsession = Session.builder.getOrCreate()\n\n# Define the table names (replace with your actual table names if different)\nTABLE_SCHEMA_REF = 'JAFFLE_LINEAGE.LINEAGE_DATA.PROCESSED_SQL_DATA'\nCOLUMN_LINEAGE_CORTEX = 'JAFFLE_LINEAGE.LINEAGE_DATA.COLUMN_LINEAGE_CORTEX_NEW'\n\n# Fetch distinct combinations of DATABASE, SCHEMA, TABLE_NAME, COLUMN_NAME, EXPANDED_SQL\ndf = session.sql(f\"\"\"\n    SELECT DISTINCT DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, EXPANDED_SQL\n    FROM {TABLE_SCHEMA_REF}\n    WHERE EXPANDED_SQL IS NOT NULL\n\"\"\")\n\nrows = df.collect()\n\nfor row in rows:\n    input_record = row.as_dict()\n    sql_query = input_record['EXPANDED_SQL']\n    \n    composite_key = {\n        'DATABASE_NAME': input_record['DATABASE_NAME'],\n        'SCHEMA_NAME': input_record['SCHEMA_NAME'],\n        'TABLE_NAME': input_record['TABLE_NAME']\n    }\n    \n    # Check if the record already exists in the target table\n    check_query = f\"\"\"\n        SELECT EXPANDED_SQL\n        FROM {COLUMN_LINEAGE_CORTEX}\n        WHERE DATABASE_NAME = '{composite_key['DATABASE_NAME']}'\n        AND SCHEMA_NAME = '{composite_key['SCHEMA_NAME']}'\n        AND TABLE_NAME = '{composite_key['TABLE_NAME']}'\n    \"\"\"\n    \n    existing_record_df = session.sql(check_query)\n    existing_records = existing_record_df.collect()\n    \n    if existing_records:\n        existing_sql = existing_records[0]['EXPANDED_SQL']\n        if existing_sql == sql_query:\n            logging.info(f\"SQL unchanged for {composite_key}. Skipping record.\")\n            print(f\"SQL unchanged for {composite_key}. Skipping record.\")\n            continue  # Skip processing if the SQL is the same\n        else:\n            logging.info(f\"SQL changed for {composite_key}. Deleting old records.\")\n            print(f\"SQL changed for {composite_key}. Deleting old records.\")\n            # SQL has changed, so delete existing records with this composite key\n            delete_query = f\"\"\"\n                DELETE FROM {COLUMN_LINEAGE_CORTEX}\n                WHERE DATABASE_NAME = '{composite_key['DATABASE_NAME']}'\n                AND SCHEMA_NAME = '{composite_key['SCHEMA_NAME']}'\n                AND TABLE_NAME = '{composite_key['TABLE_NAME']}'\n            \"\"\"\n            try:\n                session.sql(delete_query).collect()\n            except Exception as e:\n                logging.error(f\"Error deleting existing records: {e}\")\n                continue  # Skip to next record\n    else:\n        logging.info(f\"New record for {composite_key}. Processing.\")\n        print(f\"New record for {composite_key}. Processing.\")\n    \n    # Now process the SQL query using Cortex LLM to generate lineage information\n    # Construct the prompt for the Cortex LLM\n    prompt = (\n        \"You are an expert in SQL lineage analysis.\"\n        \"Given the following SQL query, identify the source tables and columns for each final column in the SELECT statement.\" \n        \"If multiple columns are used to derive the final column, please mention all columns by seperating those out using commas, Similarly of these multiple columns are coming from multiple tables, mention those as well by seperating those out using commas\"\n        \"Additionally, provide simple reasoning in business-friendly language explaining the transformation for each column. A non technical person should be able to understand\"\n        \"Provide the results as a JSON array of objects, where each object has the keys: FINAL_COLUMN, SOURCE_TABLE, SOURCE_DATABASE, SOURCE_SCHEMA, SOURCE_COLUMNS, and REASONING.\"\n        \"SOURCE_TABLE should be returned as a table name only, it shouldnt contain scheme or database name\"\n        \"SQL Query: \" + sql_query\n    )\n\n    # Escape single quotes in the prompt\n    prompt = prompt.replace(\"'\", \"''\")\n\n    # Call the Cortex LLM using the SNOWFLAKE.CORTEX.COMPLETE function\n    try:\n        lineage_response_df = session.sql(f\"\"\"\n            SELECT SNOWFLAKE.CORTEX.COMPLETE(\n                'mistral-large2',  -- You can change the model if needed\n                '{prompt}'\n            ) AS LINEAGE_RESPONSE\n        \"\"\")\n        lineage_response_row = lineage_response_df.collect()[0]\n        lineage_response = lineage_response_row['LINEAGE_RESPONSE']\n    except Exception as e:\n        logging.error(f\"Error calling Cortex LLM: {e}\")\n        print(f\"Error calling Cortex LLM: {e}\")\n        continue  # Skip to the next SQL query\n\n    # Try to extract the JSON data from the response\n    try:\n        # Use regex to extract JSON array from the response\n        json_match = re.search(r'\\[.*\\]', lineage_response, re.DOTALL)\n        if json_match:\n            json_data = json_match.group(0)\n            parsed_records = json.loads(json_data)\n        else:\n            # Try to parse the entire response if it is valid JSON\n            parsed_records = json.loads(lineage_response)\n    except json.JSONDecodeError as e:\n        logging.error(f\"Error parsing JSON response: {e}\")\n        logging.error(f\"Response: {lineage_response}\")\n        print(f\"Error parsing JSON response: {e}\")\n        continue  # Skip to the next SQL query\n\n    # Insert the parsed records into the COLUMN_LINEAGE_CORTEX table\n    for record in parsed_records:\n        # Prepare the data for insertion\n        insert_data = {\n            'DATABASE_NAME': composite_key['DATABASE_NAME'],\n            'SCHEMA_NAME': composite_key['SCHEMA_NAME'],\n            'TABLE_NAME': composite_key['TABLE_NAME'],\n            'REFERENCE': None,  # If REFERENCE is needed, you can fetch it from source if available\n            'EXPANDED_SQL': sql_query,\n            'FINAL_COLUMN': record.get('FINAL_COLUMN', 'Unknown'),\n            'SOURCE_TABLE': record.get('SOURCE_TABLE', 'Unknown'),\n            'SOURCE_DATABASE': record.get('SOURCE_DATABASE', 'Unknown'),\n            'SOURCE_SCHEMA': record.get('SOURCE_SCHEMA', 'Unknown'),\n            'SOURCE_COLUMNS': ', '.join(record.get('SOURCE_COLUMNS', [])) if isinstance(record.get('SOURCE_COLUMNS'), list) else record.get('SOURCE_COLUMNS', 'Unknown'),\n            'REASONING': record.get('REASONING', 'Unknown')\n        }\n\n        # Convert the data to a DataFrame\n        insert_df = session.create_dataframe([insert_data])\n\n        # Write the DataFrame to the COLUMN_LINEAGE_CORTEX table\n        try:\n            insert_df.write.mode('append').save_as_table(COLUMN_LINEAGE_CORTEX)\n            logging.info(f\"Inserted/Updated record for FINAL_COLUMN: {insert_data['FINAL_COLUMN']}\")\n        except Exception as e:\n            logging.error(f\"Error inserting record into {COLUMN_LINEAGE_CORTEX}: {e}\")\n            logging.error(f\"Record data: {json.dumps(insert_data)}\")\n            continue  # Skip to the next record\n\nlogging.info(\"Processing completed.\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2d3766a1-e9e3-4cad-9724-a2e63c1a55a6",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "import json\nimport pandas as pd\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\n\n# Get the active Snowpark session (in a Snowflake notebook, the session is usually already available)\nsession = Session.builder.getOrCreate()\n\n# Function to read data from Snowflake using Snowpark\ndef read_lineage_from_snowflake():\n    df_lineage = session.table(\"COLUMN_LINEAGE_CORTEX_NEW\").to_pandas()\n    \n    # Convert relevant columns to uppercase for case-insensitive matching\n    for col_name in ['TABLE_NAME', 'FINAL_COLUMN', 'SOURCE_TABLE', 'SOURCE_COLUMNS', \n                     'DATABASE_NAME', 'SCHEMA_NAME', 'TRANSFORMATION']:\n        df_lineage[col_name] = df_lineage[col_name].str.upper().str.strip()\n\n    return df_lineage\n\n# Function to write lineage records to Snowflake\ndef write_lineage_to_snowflake(lineage_records):\n    df_records = pd.DataFrame(lineage_records)\n    df_records = df_records.where(pd.notnull(df_records), None)  # Replace NaN with None\n\n    table_name = 'COLUMN_LINEAGE_MERGED'\n\n    # Truncate existing table if exists\n    try:\n        session.sql(f\"TRUNCATE TABLE {table_name}\").collect()\n    except SnowparkSQLException:\n        print(f\"Table {table_name} does not exist. Creating it...\")\n\n    # Create the table if it doesn't exist\n    session.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        SOURCE_TYPE VARCHAR(255),\n        SOURCE_NAME VARCHAR(255),\n        SOURCE_TABLE VARCHAR(255),\n        SOURCE_DATABASE VARCHAR(255),\n        TARGET_TYPE VARCHAR(255),\n        TARGET_NAME VARCHAR(255),\n        TARGET_TABLE VARCHAR(255),\n        TARGET_DATABASE VARCHAR(255),\n        TRANSFORMATION VARCHAR(16777216),\n        WORKBOOK VARCHAR(255),\n        DASHBOARD VARCHAR(255),\n        DATASOURCE VARCHAR(255),\n        SHEET VARCHAR(255)\n    )\n    \"\"\").collect()\n\n    # Write records to Snowflake using Snowpark\n    session.write_pandas(df_records, table_name, auto_create_table=False)\n\n    print(f\"Successfully wrote {len(lineage_records)} rows to {table_name}.\")\n\n# Function to process fields recursively and generate lineage records\ndef process_field(field, context, df_lineage, lineage_records, visited_nodes):\n    field_name = field.get('name', '').strip()\n    formula = field.get('formula', '').strip()\n    context['TARGET_NAME'] = field_name\n    context['TRANSFORMATION'] = formula\n    context['TARGET_TYPE'] = 'Tableau Field'\n    context['TARGET_TABLE'] = context.get('SHEET', '')\n    context['TARGET_DATABASE'] = ''  # Not applicable for Tableau fields\n\n    # Process upstreamFields recursively\n    for upstream_field in field.get('upstreamFields', []):\n        upstream_field_name = upstream_field.get('name', '').strip()\n        lineage_record = {\n            'SOURCE_TYPE': 'Tableau Field',\n            'SOURCE_NAME': upstream_field_name,\n            'SOURCE_TABLE': context.get('SHEET', ''),\n            'SOURCE_DATABASE': '',\n            'TARGET_TYPE': context['TARGET_TYPE'],\n            'TARGET_NAME': context['TARGET_NAME'],\n            'TARGET_TABLE': context['TARGET_TABLE'],\n            'TARGET_DATABASE': '',\n            'TRANSFORMATION': context['TRANSFORMATION'],\n            'WORKBOOK': context.get('WORKBOOK', ''),\n            'DASHBOARD': context.get('DASHBOARD', ''),\n            'DATASOURCE': context.get('DATASOURCE', ''),\n            'SHEET': context.get('SHEET', '')\n        }\n        lineage_records.append(lineage_record)\n        # Recursive call for upstream fields\n        process_field(upstream_field, context.copy(), df_lineage, lineage_records, visited_nodes)\n\n    # Process upstreamColumns\n    for upstream_column in field.get('upstreamColumns', []):\n        column_name = upstream_column.get('name', '').strip().upper()\n        # Process upstreamTables\n        for upstream_table in upstream_column.get('upstreamTables', []):\n            table_name = upstream_table.get('name', '').strip().upper()\n            # Get DATABASE_NAME from upstreamDatabases if available\n            databases = upstream_column.get('upstreamDatabases', [])\n            database_name = databases[0].get('name', '').strip().upper() if databases else ''\n            # Create a lineage record from database column to Tableau field\n            lineage_record = {\n                'SOURCE_TYPE': 'Database Column',\n                'SOURCE_NAME': column_name,\n                'SOURCE_TABLE': table_name,\n                'SOURCE_DATABASE': database_name,\n                'TARGET_TYPE': context['TARGET_TYPE'],\n                'TARGET_NAME': context['TARGET_NAME'],\n                'TARGET_TABLE': context['TARGET_TABLE'],\n                'TARGET_DATABASE': '',\n                'TRANSFORMATION': context['TRANSFORMATION'],\n                'WORKBOOK': context.get('WORKBOOK', ''),\n                'DASHBOARD': context.get('DASHBOARD', ''),\n                'DATASOURCE': context.get('DATASOURCE', ''),\n                'SHEET': context.get('SHEET', '')\n            }\n            lineage_records.append(lineage_record)\n            # Process database lineage\n            process_database_lineage(database_name, table_name, column_name, df_lineage, lineage_records, visited_nodes)\n\n# Function to process database lineage recursively\ndef process_database_lineage(database_name, table_name, column_name, df_lineage, lineage_records, visited_nodes):\n    node_id = f\"{database_name}.{table_name}.{column_name}\"\n    if node_id in visited_nodes:\n        # Avoid infinite recursion due to cycles\n        return\n    visited_nodes.add(node_id)\n\n    matching_rows = df_lineage[\n        (df_lineage['DATABASE_NAME'] == database_name) &\n        (df_lineage['TABLE_NAME'] == table_name) &\n        (df_lineage['FINAL_COLUMN'] == column_name)\n    ]\n    for _, row in matching_rows.iterrows():\n        source_tables = row['SOURCE_TABLE'].split(',') if row['SOURCE_TABLE'] else []\n        source_columns = row['SOURCE_COLUMNS'].split(',') if row['SOURCE_COLUMNS'] else []\n        transformations = row.get('TRANSFORMATION', '')\n        for src_table, src_column in zip(source_tables, source_columns):\n            src_table = src_table.strip().upper()\n            src_column = src_column.strip().upper()\n            lineage_record = {\n                'SOURCE_TYPE': 'Database Column',\n                'SOURCE_NAME': src_column,\n                'SOURCE_TABLE': src_table,\n                'SOURCE_DATABASE': database_name,\n                'TARGET_TYPE': 'Database Column',\n                'TARGET_NAME': column_name,\n                'TARGET_TABLE': table_name,\n                'TARGET_DATABASE': database_name,\n                'TRANSFORMATION': transformations,\n                'WORKBOOK': '',\n                'DASHBOARD': '',\n                'DATASOURCE': '',\n                'SHEET': ''\n            }\n            lineage_records.append(lineage_record)\n            # Recursive call for further database lineage\n            process_database_lineage(database_name, src_table, src_column, df_lineage, lineage_records, visited_nodes)\n\n# Main function to process the entire Tableau lineage\ndef process_tableau_lineage(tableau_data, df_lineage):\n    lineage_records = []\n    visited_nodes = set()\n\n    for workbook in tableau_data.get('workbooks', []):\n        context = {'WORKBOOK': workbook.get('name', '').strip()}\n        for dashboard in workbook.get('dashboards', []):\n            context['DASHBOARD'] = dashboard.get('name', '').strip()\n            for datasource in dashboard.get('upstreamDatasources', []):\n                context['DATASOURCE'] = datasource.get('name', '').strip()\n                for sheet in datasource.get('sheets', []):\n                    context['SHEET'] = sheet.get('name', '').strip()\n                    for field in sheet.get('upstreamFields', []):\n                        process_field(field, context.copy(), df_lineage, lineage_records, visited_nodes)\n\n    return lineage_records\n\n# Function to process all database lineage and generate lineage records\ndef process_all_database_lineage(df_lineage):\n    lineage_records = []\n    visited_nodes = set()\n\n    # Get unique database, table, column combinations\n    unique_columns = df_lineage[['DATABASE_NAME', 'TABLE_NAME', 'FINAL_COLUMN']].drop_duplicates()\n\n    for _, row in unique_columns.iterrows():\n        database_name = row['DATABASE_NAME']\n        table_name = row['TABLE_NAME']\n        column_name = row['FINAL_COLUMN']\n        process_database_lineage(database_name, table_name, column_name, df_lineage, lineage_records, visited_nodes)\n\n    return lineage_records\n\n\n# Main execution function\ndef main():\n    # Load Tableau lineage\n    with open('tableau_lineage.json', 'r') as f:\n        tableau_data = json.load(f)\n\n    # Read database lineage from Snowflake\n    df_lineage = read_lineage_from_snowflake()\n\n    # Process the Tableau lineage data\n    tableau_lineage_records = process_tableau_lineage(tableau_data, df_lineage)\n\n    # Process all database lineage data\n    database_lineage_records = process_all_database_lineage(df_lineage)\n\n    # Combine the two sets of lineage records\n    lineage_records = tableau_lineage_records + database_lineage_records\n\n    # Write the lineage records to Snowflake\n    write_lineage_to_snowflake(lineage_records)\n\nif __name__ == \"__main__\":\n    main()\n",
   "execution_count": null
  }
 ]
}